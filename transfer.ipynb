{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import scipy.misc\n",
    "import os\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "from scipy.misc import imresize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros')\n",
    "        # In the simplest case, the output value of the layer with input size (N,Cin,H,W)(N, C_{\\text{in}}, H, W)(N,Cin​,H,W) and output (N,Cout,Hout,Wout)(N, C_{\\text{out}}, H_{\\text{out}}, W_{\\text{out}})(N,Cout​,Hout​,Wout​) \n",
    "        self.conv1 = nn.Conv2d(3, 32, 5, 1, 2)\n",
    "        self.pool1 = nn.MaxPool2d(3,2, ceil_mode = True)\n",
    "        self.conv2 = nn.Conv2d(32, 16, 5, 1, 2)\n",
    "        self.pool2 = nn.AvgPool2d(3, 2, ceil_mode = True)\n",
    "        self.conv3 = nn.Conv2d(16, 32, 5, 1, 2)\n",
    "        self.pool3 = nn.AvgPool2d(3, 2, ceil_mode = True)\n",
    "        self.fc1 = nn.Linear(32*4*4, 10)\n",
    "        # self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        # self.pool = nn.MaxPool2d(2, 2)\n",
    "        # self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        # self.fc2 = nn.Linear(120, 84)\n",
    "        # self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        t1 = F.relu(self.pool1(self.conv1(x)))\n",
    "        t2 = self.pool2(F.relu(self.conv2(t1)))\n",
    "        t3 = self.pool3(F.relu(self.conv3(t2)))\n",
    "        x = t3.view(-1, 32 * 4 * 4)\n",
    "        x = self.fc1(x)\n",
    "        # x = F.relu(self.fc2(x))\n",
    "        # x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yaohsiao/anaconda3/envs/cs231n/lib/python3.6/site-packages/torch/serialization.py:435: SourceChangeWarning: source code of class 'torch.nn.modules.conv.Conv2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/Users/yaohsiao/anaconda3/envs/cs231n/lib/python3.6/site-packages/torch/serialization.py:435: SourceChangeWarning: source code of class 'torch.nn.modules.linear.Linear' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "net = torch.load('save_adam_61.pt',map_location=device)\n",
    "correct = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as fixed feature extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    }
   ],
   "source": [
    "num_ftrs = net.fc1.in_features\n",
    "print(num_ftrs)\n",
    "net.fc1 = nn.Linear(num_ftrs, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 32, 32])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "values not supported on torch.FloatTensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-163831fc5128>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtm\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: values not supported on torch.FloatTensor"
     ]
    }
   ],
   "source": [
    "# im = Image.open(os.path.join(data_base,\"1528642314.jpg\"))\n",
    "# transform = transforms.Compose(\n",
    "#     [\n",
    "#     transforms.Resize((32,32)),\n",
    "#      transforms.ToTensor(),\n",
    "#      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "# tm  = transform(im)\n",
    "# print(tm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "labels = pickle.load(open(\"mylabel_2.pkl\", \"rb\")) # dictionary\n",
    "data_base = \"../mydata\"\n",
    "img_files = [data for data in os.listdir(data_base) if data.endswith(\".jpg\")]\n",
    "valid_img_files = list(set(img_files) & set(labels.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmp = scipy.misc.imread(os.path.join(data_base,\"1528642314.jpg\"))\n",
    "# print(type(tmp), tmp.shape)\n",
    "# img = torch.from_numpy(tmp)\n",
    "# img = img[np.newaxis, :]\n",
    "# img = img.permute(0,3,1,2).squeeze(0)\n",
    "# print(img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_data = []\n",
    "labels_data = []\n",
    "target_size = (32, 32)\n",
    "i = 0\n",
    "for img_name in valid_img_files:\n",
    "    im = Image.open(os.path.join(data_base,img_name))\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "        transforms.Resize((32,32)),\n",
    "         transforms.ToTensor(),\n",
    "         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "    tm  = transform(im)\n",
    "\n",
    "# tmp = scipy.misc.imread(os.path.join(data_base,\"1528642314.jpg\"))\n",
    "#     tmp = imresize(tmp, target_size)\n",
    "#     img = torch.from_numpy(tmp)\n",
    "#     img = img[np.newaxis, :]\n",
    "#     img = img.permute(0,3,1,2).squeeze(0)\n",
    "    images_data.append(tm)\n",
    "    if labels[img_name] == '1':\n",
    "        labels_data.append(1) \n",
    "    else:\n",
    "        labels_data.append(0)\n",
    "        \n",
    "#     img = scipy.misc.imread(os.path.join(self.data_base,img_name))\n",
    "#     img = imresize(img,target_size)\n",
    "#     if len(pos_data) >= pos_num and len(neg_data) >= neg_num:\n",
    "#         break\n",
    "#     if self.label[img_name] == '1':\n",
    "#         if len(pos_data) < pos_num :\n",
    "#             pos_data.append(img)\n",
    "#             pbar.update(1)\n",
    "#     else :\n",
    "#         if len(neg_data) < neg_num :\n",
    "#             neg_data.append(img)\n",
    "#             pbar.update(1)\n",
    "# pbar.close()\n",
    "# data = pos_data + neg_data\n",
    "# data = np.stack(data,axis=0)\n",
    "# label = np.concatenate((np.ones(pos_num),np.zeros(neg_num)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "torch.Size([411, 3, 32, 32])\n",
      "411\n",
      "torch.Size([3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "print(type(images_data))\n",
    "stacked_images = torch.stack(images_data)\n",
    "print(stacked_images.shape)\n",
    "labels_data = torch.from_numpy(np.array(labels_data))\n",
    "print(len(images_data))\n",
    "print((images_data[0].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     1] loss: 0.000\n",
      "[1,    51] loss: 0.000\n",
      "[1,   101] loss: 0.001\n",
      "[1,   151] loss: 0.000\n",
      "[1,   201] loss: 0.001\n",
      "[1,   251] loss: 0.001\n",
      "[1,   301] loss: 0.000\n",
      "[1,   351] loss: 0.000\n",
      "[1,   401] loss: 0.000\n",
      "[2,     1] loss: 0.000\n",
      "[2,    51] loss: 0.000\n",
      "[2,   101] loss: 0.001\n",
      "[2,   151] loss: 0.000\n",
      "[2,   201] loss: 0.001\n",
      "[2,   251] loss: 0.001\n",
      "[2,   301] loss: 0.000\n",
      "[2,   351] loss: 0.000\n",
      "[2,   401] loss: 0.000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    i = 0\n",
    "    while i+1 < 411:\n",
    "#     for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs\n",
    "#         inputs, labels = data\n",
    "        inputs = torch.stack([images_data[i], images_data[i+1]])\n",
    "    \n",
    "#         print(inputs.shape)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "#         print(labels_data[i])\n",
    "        t = torch.ones(2, dtype=torch.int64)\n",
    "        t[0] = labels_data[i]\n",
    "        t[1] = labels_data[i+1]\n",
    "        loss = criterion(outputs, t)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 50 == 0:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "#         print(i)\n",
    "        i = i + 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(0,-1,-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
